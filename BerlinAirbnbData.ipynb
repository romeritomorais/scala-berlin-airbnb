{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Berlin Airbnb Data\n",
    "Investigating Airbnb activity in Berlin, Germany"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined object datasets\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object datasets {\n",
    "    val reviews_summary = \"reviews_summary.csv\"\n",
    "};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReviewsSummaryDF: org.apache.spark.sql.DataFrame = [listing_id: string, id: string ... 4 more fields]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// load dataset reviews_summary\n",
    "val ReviewsSummaryDF = spark.read.option(\"inferSchema\", true)\n",
    "                                 .option(\"header\", \"true\")\n",
    "                                 .csv(datasets.reviews_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RemoverNullDF: org.apache.spark.sql.DataFrame = [listing_id: string, id: string ... 4 more fields]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// remove linhas vazias\n",
    "val RemoverNullDF = ReviewsSummaryDF.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- listing_id: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- reviewer_id: string (nullable = true)\n",
      " |-- reviewer_name: string (nullable = true)\n",
      " |-- comments: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// exibe o squena do dataframe\n",
    "RemoverNullDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "405907 linhas\n"
     ]
    }
   ],
   "source": [
    "// exibe o numero de linhas do dataframe\n",
    "println(RemoverNullDF.count()+\" linhas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NewDF: org.apache.spark.sql.DataFrame = [listing_id: string, data: date ... 2 more fields]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// cria novo dataframe renomeando colunas e definindo o tipo de dados do campo data\n",
    "val NewDF = RemoverNullDF.select(\"listing_id\", \"date\", \"reviewer_name\", \"comments\")\n",
    "                                .withColumn(\"date\", col(\"date\")\n",
    "                                .cast(\"date\"))\n",
    "                                .withColumnRenamed(\"date\", \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- listing_id: string (nullable = true)\n",
      " |-- data: date (nullable = true)\n",
      " |-- reviewer_name: string (nullable = true)\n",
      " |-- comments: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// exibe o squena do dataframe\n",
    "NewDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------------+--------------------+\n",
      "|listing_id|      data|reviewer_name|            comments|\n",
      "+----------+----------+-------------+--------------------+\n",
      "|      2015|2016-04-11|        Rahel|Mein Freund und i...|\n",
      "|      2015|2016-04-15|       Hannah|Jan was very frie...|\n",
      "|      2015|2016-04-26|       Victor|Un appartement tr...|\n",
      "|      2015|2016-05-10|         Judy|It is really nice...|\n",
      "|      2015|2016-05-14|       Romina|Buena ubicación, ...|\n",
      "+----------+----------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NewDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "// cria tabela temporária\n",
    "NewDF.createOrReplaceTempView(\"table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RemoveDuplicatesDF: org.apache.spark.sql.DataFrame = [year: int, month: int ... 3 more fields]\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// cria novo dataframe a partir de consulta sql removendo campos vazios\n",
    "val RemoveDuplicatesDF = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    year(data) AS year,\n",
    "    month(data) AS month,\n",
    "    day(data) AS day,\n",
    "    reviewer_name AS name,\n",
    "    comments\n",
    "FROM table\n",
    "ORDER BY year\n",
    "\"\"\").na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+--------+--------------------+\n",
      "|year|month|day|    name|            comments|\n",
      "+----+-----+---+--------+--------------------+\n",
      "|2009|    6| 20|   Milan|excellent stay, i...|\n",
      "|2009|    8| 18|     Ben|I could not have ...|\n",
      "|2009|   11| 25|  Tarnia|This room is real...|\n",
      "|2010|    8| 24|  Yuliya|Thank you very mu...|\n",
      "|2010|   11| 24|Patricia|Fantastic, large ...|\n",
      "+----+-----+---+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "RemoveDuplicatesDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- comments: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "RemoveDuplicatesDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "// salva do dados no formato parquet particionando por year\n",
    "RemoveDuplicatesDF.write.partitionBy(\"year\")\n",
    "                  .format(\"parquet\")\n",
    "                  .mode(\"overwrite\")\n",
    "                  .save(\"partition_year.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "partition_year: String = /home/romerito/Dropbox/tecnology/bigdata_analytics/datasets/partition_year.parquet\n",
       "Load_ParquetDF: org.apache.spark.sql.DataFrame = [month: int, day: int ... 3 more fields]\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// carrega os dados de arquivos parquet\n",
    "val partition_year = \"/home/romerito/Dropbox/tecnology/bigdata_analytics/datasets/partition_year.parquet\"\n",
    "val Load_ParquetDF = spark.read.load(partition_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "// cria tabela temporária\n",
    "Load_ParquetDF.createOrReplaceTempView(\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountLinesPartitionDF: org.apache.spark.sql.DataFrame = [year: int, lines_partition: bigint]\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// cria novo dataframe com dados agrupados do numero de linhas por year\n",
    "val CountLinesPartitionDF = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT\n",
    "    year,\n",
    "    count(*) AS lines_partition\n",
    "    FROM parquet\n",
    "    GROUP BY year\n",
    "    \"\"\"\n",
    "    ).na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------+\n",
      "|year|lines_partition|\n",
      "+----+---------------+\n",
      "|2018|         152182|\n",
      "|2015|          39467|\n",
      "|2013|           7422|\n",
      "|2014|          17683|\n",
      "|2012|           3092|\n",
      "|2009|              3|\n",
      "|2016|          69216|\n",
      "|2010|            106|\n",
      "|2011|            617|\n",
      "|2017|         111692|\n",
      "+----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "CountLinesPartitionDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "// salva dados no formato parquet\n",
    "RemoveDuplicatesDF.write.partitionBy(\"year\")\n",
    "                  .format(\"parquet\")\n",
    "                  .mode(\"overwrite\")\n",
    "                  .save(\"CountLinesPartition.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Spark 3",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
